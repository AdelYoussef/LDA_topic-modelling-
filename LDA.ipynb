{"cells":[{"cell_type":"code","source":["\n","from sklearn.datasets import fetch_20newsgroups\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import numpy as np\n","import pandas as pd\n","np.random.seed(400)\n","import nltk\n","\n","\n","\n","newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n","newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)\n","\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')\n","stemmer = SnowballStemmer(\"english\")\n","\n","\n","class LDA :\n","        \n","        \n","    def init(self,train , test):\n","        self.data_train = train\n","        self.data_test = test\n","        \n","        nltk.download('omw-1.4')\n","        nltk.download('wordnet')\n","        self.stemmer = SnowballStemmer(\"english\")\n","        \n","        return self.data_train, self.data_test\n","    \n","        \n","    def parsing(self, data):\n","        result=[]\n","        for token in gensim.utils.simple_preprocess(data) :\n","            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","                \n","                Lemmatizer = WordNetLemmatizer().lemmatize(token, pos = 'v') \n","                stemmer_Lemmatizer =  self.stemmer.stem(Lemmatizer)\n","                result.append(stemmer_Lemmatizer)\n","                \n","        return result\n","    \n","    \n","    def preprocess(self, data):\n","        \n","        processed_docs = []\n","    \n","        for doc in data:\n","            processed_docs.append(self.parsing(doc))\n","            \n","        return processed_docs \n","\n","    \n","    def doc2bow_dict(self,data):\n","        self.dictionary = gensim.corpora.Dictionary(data)\n","        self.bow_corpus = [self.dictionary.doc2bow(doc) for doc in data]\n","        return self.bow_corpus, self.dictionary\n","    \n","\n","    def filter_docs(self,min_reps , min_occr , keep_most_freq = None):\n","\n","\n","        self.dictionary.filter_extremes(no_below = min_reps, no_above = min_occr , keep_n= keep_most_freq)\n","        \n","        \n","        \n","    def LDA_model(self,num_topics , dic, epochs, cores=2):\n","        \n","        if self.multicore == True:\n","            print(\"training begins ++++++ using multicore \\n\")\n","            self.lda_model =  gensim.models.LdaMulticore(self.bow_corpus, num_topics = num_topics, id2word = dic, passes = epochs, workers = cores)\n","            \n","        elif self.multicore == False:\n","            print(\"training begins ++++++ using singlecore \\n\")\n","            self.lda_model = gensim.models.LdaModel(self.bow_corpus, num_topics = num_topics, id2word = dic, passes = epochs)\n","            \n","        return self.lda_model\n","        \n","    def save_model(self,path):\n","      path = path + \"LDA_model\"\n","      self.lda_model.save(path)\n","\n","    def train(self, path = \"\", save_model = True , Filter = True , multicore = True , cores = 2 , batch_size = 10 , epochs = 50 ,  num_topics=10 ):\n","        self.multicore = multicore\n","        self.Filter = Filter\n","        self.processed_docs = self.preprocess(self.data_train)\n","        \n","        self.doc2bow_dict(self.processed_docs)\n","        \n","        \n","        if self.Filter == True:\n","            self.filter_docs(min_reps = 15, min_occr = 0.1, keep_most_freq = None)\n","        \n","        \n","        self.LDA_model(num_topics = num_topics, dic = self.dictionary, epochs = epochs)\n","        print(\"training successful \\n\")\n","\n","        if save_model == True:\n","            self.save_model(path)\n","        \n","    def classify(self,data):\n","        predict = []\n","        for doc in (data):\n","          processed = self.parsing(doc)\n","          bow_vector = self.dictionary.doc2bow(processed)\n","          vector =  np.array(self.lda_model[bow_vector])\n","          vec = np.max(vector, axis=0)\n","          predict.append(vec)\n","          \n","        return predict     \n","        \n","       \n","            \n","train = fetch_20newsgroups(subset='train', shuffle = True)\n","test = fetch_20newsgroups(subset='test', shuffle = True)\n","  \n","mylda = LDA()\n","\n","mytrain , mytest = mylda.init(train.data , test.data)\n","\n","\n","mylda.train(Filter = False , multicore = False , batch_size = 10 , epochs = 50 ,num_topics=10)\n","\n","\n","unseen_document = []\n","unseen_document.append(newsgroups_test.data[100])\n","unseen_document.append(newsgroups_test.data[50])\n","unseen_document.append(newsgroups_test.data[60])\n","\n","\n","\n","\n","\n","    "],"metadata":{"id":"hW6gctwa2GP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = mylda.classify(unseen_document)\n"],"metadata":{"id":"vOyh_AhJ2PZT"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"LDA.ipynb","provenance":[],"authorship_tag":"ABX9TyP8X3B7jRQrQH9ZZCzlLcms"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}